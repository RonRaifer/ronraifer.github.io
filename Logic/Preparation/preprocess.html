<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>Logic.Preparation.preprocess API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Logic.Preparation.preprocess</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import html
import logging
import re

import pyarabic.araby as araby

ACCEPTED_MODELS = [
    &#34;bert-base-arabertv01&#34;,
    &#34;bert-base-arabert&#34;,
    &#34;bert-base-arabertv02&#34;,
    &#34;bert-base-arabertv2&#34;,
    &#34;bert-large-arabertv02&#34;,
    &#34;bert-large-arabertv2&#34;,
    &#34;araelectra-base&#34;,
    &#34;araelectra-base-discriminator&#34;,
    &#34;araelectra-base-generator&#34;,
    &#34;aragpt2-base&#34;,
    &#34;aragpt2-medium&#34;,
    &#34;aragpt2-large&#34;,
    &#34;aragpt2-mega&#34;,
]

SEGMENTED_MODELS = [
    &#34;bert-base-arabert&#34;,
    &#34;bert-base-arabertv2&#34;,
    &#34;bert-large-arabertv2&#34;,
]


class ArabertPreprocessor:
    &#34;&#34;&#34;
    A Preprocessor class that cleans and preprocesses text for all models in the AraBERT repo.
    It also can unprocess the text ouput of the generated text

    Args:

        model_name (:obj:`str`): model name from the HuggingFace Models page with or without `aubmindlab/`. Current accepted models are:

            - :obj:`&#34;bert-base-arabertv01&#34;`: No farasa segmentation.
            - :obj:`&#34;bert-base-arabert&#34;`: with farasa segmentation.
            - :obj:`&#34;bert-base-arabertv02&#34;`: No farasas egmentation.
            - :obj:`&#34;bert-base-arabertv2&#34;`: with farasa segmentation.
            - :obj:`&#34;bert-large-arabertv02&#34;`: No farasas egmentation.
            - :obj:`&#34;bert-large-arabertv2&#34;`: with farasa segmentation.
            - :obj:`&#34;araelectra-base&#34;`: No farasa segmentation.
            - :obj:`&#34;araelectra-base-discriminator&#34;`: No farasa segmentation.
            - :obj:`&#34;araelectra-base-generator&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-base&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-medium&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-large&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-mega&#34;`: No farasa segmentation.

    keep_emojis(:obj: `bool`): don&#39;t remove emojis while preprocessing. Defaults to False

    remove_html_markup(:obj: `bool`): Whether to remove html artfacts, should be set to False when preprocessing TyDi QA. Defaults to True

    replace_urls_emails_mentions(:obj: `bool`): Whether to replace email urls and mentions by special tokens. Defaults to True

    Returns:

        ArabertPreprocessor: the preprocessor class

    Example:

        from preprocess import ArabertPreprocessor

        arabert_prep = ArabertPreprocessor(&#34;bert-base-arabertv2&#34;)
        arabert_prep.preprocess(&#34;SOME ARABIC TEXT&#34;)
    &#34;&#34;&#34;

    def __init__(
        self,
        model_name,
        keep_emojis=False,
        remove_html_markup=True,
        replace_urls_emails_mentions=True,
    ):
        &#34;&#34;&#34;
        model_name (:obj:`str`): model name from the HuggingFace Models page without the aubmindlab tag. Current accepted models are:

            - :obj:`&#34;bert-base-arabertv01&#34;`: No farasa segmentation.
            - :obj:`&#34;bert-base-arabert&#34;`: with farasa segmentation.
            - :obj:`&#34;bert-base-arabertv02&#34;`: No farasas egmentation.
            - :obj:`&#34;bert-base-arabertv2&#34;`: with farasa segmentation.
            - :obj:`&#34;bert-large-arabertv02&#34;`: No farasas egmentation.
            - :obj:`&#34;bert-large-arabertv2&#34;`: with farasa segmentation.
            - :obj:`&#34;araelectra-base&#34;`: No farasa segmentation.
            - :obj:`&#34;araelectra-base-discriminator&#34;`: No farasa segmentation.
            - :obj:`&#34;araelectra-base-generator&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-base&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-medium&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-large&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-mega&#34;`: No farasa segmentation.

        keep_emojis(:obj: `bool`): don&#39;t remove emojis while preprocessing. Defaults to False

        remove_html_markup(:obj: `bool`): Whether to remove html artfacts, should be set to False when preprocessing TyDi QA. Defaults to True

        replace_urls_emails_mentions(:obj: `bool`): Whether to replace email urls and mentions by special tokens. Defaults to True
        &#34;&#34;&#34;
        model_name = model_name.replace(&#34;aubmindlab/&#34;, &#34;&#34;)

        if model_name not in ACCEPTED_MODELS:
            logging.warning(
                &#34;Model provided is not in the accepted model list. Assuming you don&#39;t want Farasa Segmentation&#34;
            )
            self.model_name = &#34;bert-base-arabertv02&#34;
        else:
            self.model_name = model_name

        if self.model_name in SEGMENTED_MODELS:
            logging.info(
                &#34;Selected Model requires pre-segmentation, Initializing FarasaSegmenter&#34;
            )
            try:
                from farasa.segmenter import FarasaSegmenter

                self.farasa_segmenter = FarasaSegmenter(interactive=True)
            except:
                logging.warning(
                    &#34;farasapy is not installed, you want be able to process text for AraBERTv1 and v2. Install it using: pip install farasapy&#34;
                )
        else:
            logging.info(
                &#34;Selected Model doesn&#39;t require pre-segmentation, skipping FarasaSegmenter initialization&#34;
            )

        self.keep_emojis = keep_emojis
        if self.keep_emojis:
            import emoji

            self.emoji = emoji
            if self.model_name in SEGMENTED_MODELS:
                logging.warning(
                    &#34;Keeping tweets with Farasa Segmentation is 10 times slower&#34;
                )

        self.remove_html_markup = remove_html_markup
        self.replace_urls_emails_mentions = replace_urls_emails_mentions

    def preprocess(self, text):
        &#34;&#34;&#34;
        Preprocess takes an input text line an applies the same preprocessing used in AraBERT
                            pretraining

        Args:

            text (:obj:`str`): inout text string

        Returns:

            string: A preprocessed string depending on which model was selected
        &#34;&#34;&#34;
        if self.model_name == &#34;bert-base-arabert&#34;:
            return self._old_preprocess(
                text,
                do_farasa_tokenization=True,
            )

        if self.model_name == &#34;bert-base-arabertv01&#34;:
            return self._old_preprocess(text, do_farasa_tokenization=False)

        text = str(text)
        text = html.unescape(text)
        text = araby.strip_tashkeel(text)
        text = araby.strip_tatweel(text)

        if self.replace_urls_emails_mentions:
            # replace all possible URLs
            for reg in url_regexes:
                text = re.sub(reg, &#34; [رابط] &#34;, text)
            # REplace Emails with [بريد]
            for reg in email_regexes:
                text = re.sub(reg, &#34; [بريد] &#34;, text)
            # replace mentions with [مستخدم]
            text = re.sub(user_mention_regex, &#34; [مستخدم] &#34;, text)

        if self.remove_html_markup:
            # remove html line breaks
            text = re.sub(&#34;&lt;br /&gt;&#34;, &#34; &#34;, text)
            # remove html markup
            text = re.sub(&#34;&lt;/?[^&gt;]+&gt;&#34;, &#34; &#34;, text)
        # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets
        text = re.sub(
            &#34;([^0-9\u0621-\u063A\u0641-\u064A\u0660-\u0669a-zA-Z\[\]])&#34;, r&#34; \1 &#34;, text
        )

        # insert whitespace between words and numbers or numbers and words
        text = re.sub(
            &#34;(\d+)([\u0621-\u063A\u0641-\u064A\u0660-\u066C]+)&#34;, r&#34; \1 \2 &#34;, text
        )
        text = re.sub(
            &#34;([\u0621-\u063A\u0641-\u064A\u0660-\u066C]+)(\d+)&#34;, r&#34; \1 \2 &#34;, text
        )

        # remove unwanted characters
        if self.keep_emojis:
            emoji_regex = &#34;&#34;.join(list(self.emoji.UNICODE_EMOJI[&#34;en&#34;].keys()))
            rejected_chars_regex2 = &#34;[^%s%s]&#34; % (chars_regex, emoji_regex)
            text = re.sub(rejected_chars_regex2, &#34; &#34;, text)
        else:
            text = re.sub(rejected_chars_regex, &#34; &#34;, text)

        # remove repeated characters &gt;2
        text = self._remove_elongation(text)
        # remove extra spaces
        text = &#34; &#34;.join(text.replace(&#34;\uFE0F&#34;, &#34;&#34;).split())

        if (
            self.model_name == &#34;bert-base-arabertv2&#34;
            or self.model_name == &#34;bert-large-arabertv2&#34;
        ):
            if self.keep_emojis:
                new_text = []
                for word in text.split():
                    if word in list(self.emoji.UNICODE_EMOJI[&#34;en&#34;].keys()):
                        new_text.append(word)
                    else:
                        new_text.append(self.farasa_segmenter.segment(word))
                text = &#34; &#34;.join(new_text)
            else:
                text = self.farasa_segmenter.segment(text)
            return self._farasa_segment(text)

        # ALl the other models dont require Farasa Segmentation
        return text

    def unpreprocess(self, text, desegment=True):
        &#34;&#34;&#34;Re-formats the text to a classic format where punctuations, brackets, parenthesis are not seperated by whitespaces.
        The objective is to make the generated text of any model appear natural and not preprocessed.

        Args:
            text (str): input text to be un-preprocessed
            desegment (bool, optional): [whether or not to remove farasa pre-segmentation before]. Defaults to True.

        Returns:
            str: The unpreprocessed (and possibly Farasa-desegmented) text.
        &#34;&#34;&#34;

        if self.model_name in SEGMENTED_MODELS and desegment:
            text = self.desegment(text)

        # removes the spaces around quotation marks ex: i &#34; ate &#34; an apple --&gt; i &#34;ate&#34; an apple
        # https://stackoverflow.com/a/53436792/5381220
        text = re.sub(white_spaced_double_quotation_regex, &#39;&#34;&#39; + r&#34;\1&#34; + &#39;&#34;&#39;, text)
        text = re.sub(white_spaced_single_quotation_regex, &#34;&#39;&#34; + r&#34;\1&#34; + &#34;&#39;&#34;, text)
        text = re.sub(white_spaced_back_quotation_regex, &#34;\`&#34; + r&#34;\1&#34; + &#34;\`&#34;, text)
        text = re.sub(white_spaced_back_quotation_regex, &#34;\—&#34; + r&#34;\1&#34; + &#34;\—&#34;, text)

        # during generation, sometimes the models don&#39;t put a space after the dot, this handles it
        text = text.replace(&#34;.&#34;, &#34; . &#34;)
        text = &#34; &#34;.join(text.split())

        # handle decimals
        text = re.sub(r&#34;(\d+) \. (\d+)&#34;, r&#34;\1.\2&#34;, text)
        text = re.sub(r&#34;(\d+) \, (\d+)&#34;, r&#34;\1,\2&#34;, text)

        text = re.sub(left_and_right_spaced_chars, r&#34;\1&#34;, text)
        text = re.sub(left_spaced_chars, r&#34;\1&#34;, text)
        text = re.sub(right_spaced_chars, r&#34;\1&#34;, text)

        return text

    def desegment(self, text):
        &#34;&#34;&#34;
        Use this function if sentence tokenization was done using
        `from arabert.preprocess_arabert import preprocess` with Farasa enabled
        AraBERT segmentation using Farasa adds a space after the &#39;+&#39; for prefixes,
        and after before the &#39;+&#39; for suffixes

        Example:
        &gt;&gt;&gt; desegment(&#39;ال+ دراس +ات&#39;)
        الدراسات
        &#34;&#34;&#34;
        text = text.replace(&#34;+ &#34;, &#34;+&#34;)
        text = text.replace(&#34; +&#34;, &#34;+&#34;)
        text = &#34; &#34;.join([self._desegmentword(word) for word in text.split(&#34; &#34;)])
        return text

    def _desegmentword(self, orig_word: str) -&gt; str:
        &#34;&#34;&#34;
        Word segmentor that takes a Farasa Segmented Word and removes the &#39;+&#39; signs

        Example:
        &gt;&gt;&gt; _desegmentword(&#34;ال+يومي+ة&#34;)
        اليومية
        &#34;&#34;&#34;
        word = orig_word.replace(&#34;ل+ال+&#34;, &#34;لل&#34;)
        if &#34;ال+ال&#34; not in orig_word:
            word = word.replace(&#34;ل+ال&#34;, &#34;لل&#34;)
        word = word.replace(&#34;+&#34;, &#34;&#34;)
        word = word.replace(&#34;للل&#34;, &#34;لل&#34;)
        return word

    def _old_preprocess(self, text, do_farasa_tokenization):
        &#34;&#34;&#34;
        AraBERTv1 preprocessing Function
        &#34;&#34;&#34;
        text = str(text)
        text = araby.strip_tashkeel(text)
        text = re.sub(r&#34;\d+\/[ء-ي]+\/\d+\]&#34;, &#34;&#34;, text)
        text = re.sub(&#34;ـ&#34;, &#34;&#34;, text)
        text = re.sub(&#34;[«»]&#34;, &#39; &#34; &#39;, text)

        if self.replace_urls_emails_mentions:
            # replace the [رابط] token with space if you want to clean links
            text = re.sub(regex_url_step1, &#34;[رابط]&#34;, text)
            text = re.sub(regex_url_step2, &#34;[رابط]&#34;, text)
            text = re.sub(regex_url, &#34;[رابط]&#34;, text)
            text = re.sub(regex_email, &#34;[بريد]&#34;, text)
            text = re.sub(regex_mention, &#34;[مستخدم]&#34;, text)
        text = re.sub(&#34;…&#34;, r&#34;arabert&#34;, text).strip()
        text = self._remove_redundant_punct(text)

        if self.replace_urls_emails_mentions:
            text = re.sub(r&#34;\[ رابط \]|\[ رابط\]|\[رابط \]&#34;, &#34; [رابط] &#34;, text)
            text = re.sub(r&#34;\[ بريد \]|\[ بريد\]|\[بريد \]&#34;, &#34; [بريد] &#34;, text)
            text = re.sub(r&#34;\[ مستخدم \]|\[ مستخدم\]|\[مستخدم \]&#34;, &#34; [مستخدم] &#34;, text)

        text = self._remove_elongation(text)
        text = re.sub(
            &#34;([^0-9\u0621-\u063A\u0641-\u0669\u0671-\u0673a-zA-Z\[\]])&#34;, r&#34; \1 &#34;, text
        )
        if do_farasa_tokenization:
            text = self._tokenize_arabic_words_farasa(text)

        return text.strip()

    def _farasa_segment(self, text):
        line_farasa = text.split()
        segmented_line = []
        for index, word in enumerate(line_farasa):
            if word in [&#34;[&#34;, &#34;]&#34;]:
                continue
            if word in [&#34;رابط&#34;, &#34;بريد&#34;, &#34;مستخدم&#34;] and line_farasa[index - 1] in [
                &#34;[&#34;,
                &#34;]&#34;,
            ]:
                segmented_line.append(&#34;[&#34; + word + &#34;]&#34;)
                continue
            if &#34;+&#34; not in word:
                segmented_line.append(word)
                continue
            segmented_word = self._split_farasa_output(word)
            segmented_line.extend(segmented_word)

        return &#34; &#34;.join(segmented_line)

    def _split_farasa_output(self, word):
        segmented_word = []
        temp_token = &#34;&#34;
        for i, c in enumerate(word):
            if c == &#34;+&#34;:
                # if the token is KAF, it could be a suffix or prefix
                if temp_token == &#34;ك&#34;:
                    # if we are at the second token, then KAF is surely a prefix
                    if i == 1:
                        segmented_word.append(temp_token + &#34;+&#34;)
                        temp_token = &#34;&#34;
                    # If the KAF token is between 2 tokens
                    elif word[i - 2] == &#34;+&#34;:
                        # if the previous token is prefix, then this KAF must be a prefix
                        if segmented_word[-1][-1] == &#34;+&#34;:
                            segmented_word.append(temp_token + &#34;+&#34;)
                            temp_token = &#34;&#34;
                        # else it is a suffix, this KAF could not be a second suffix
                        else:
                            segmented_word.append(&#34;+&#34; + temp_token)
                            temp_token = &#34;&#34;
                    # if Kaf is at the end, this is handled with the statement after the loop
                elif temp_token in prefix_list:
                    segmented_word.append(temp_token + &#34;+&#34;)
                    temp_token = &#34;&#34;
                elif temp_token in suffix_list:
                    segmented_word.append(&#34;+&#34; + temp_token)
                    temp_token = &#34;&#34;
                else:
                    segmented_word.append(temp_token)
                    temp_token = &#34;&#34;
                continue
            temp_token += c
        if temp_token != &#34;&#34;:
            if temp_token in suffix_list:
                segmented_word.append(&#34;+&#34; + temp_token)
            else:
                segmented_word.append(temp_token)
        return segmented_word

    def _tokenize_arabic_words_farasa(self, line_input):

        if self.keep_emojis:
            # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets
            line_farasa = []
            for word in line_input.split():
                if word in list(self.emoji.UNICODE_EMOJI[&#34;en&#34;].keys()):
                    line_farasa.append(word)
                else:
                    line_farasa.append(self.farasa_segmenter.segment(word))
        else:
            line_farasa = self.farasa_segmenter.segment(line_input).split()

        segmented_line = []
        for index, word in enumerate(line_farasa):
            if word in [&#34;[&#34;, &#34;]&#34;]:
                continue
            if word in [&#34;رابط&#34;, &#34;بريد&#34;, &#34;مستخدم&#34;] and line_farasa[index - 1] in [
                &#34;[&#34;,
                &#34;]&#34;,
            ]:
                segmented_line.append(&#34;[&#34; + word + &#34;]&#34;)
                continue
            segmented_word = []
            for token in word.split(&#34;+&#34;):
                if token in prefix_list:
                    segmented_word.append(token + &#34;+&#34;)
                elif token in suffix_list:
                    segmented_word.append(&#34;+&#34; + token)
                else:
                    segmented_word.append(token)
            segmented_line.extend(segmented_word)
        return &#34; &#34;.join(segmented_line)

    def _remove_elongation(self, word):
        &#34;&#34;&#34;
        :param word:  the input word to remove elongation
        :return: delongated word
        &#34;&#34;&#34;
        # loop over the number of times the regex matched the word
        for index_ in range(len(re.findall(regex_tatweel, word))):
            if re.search(regex_tatweel, word):
                elongation_found = re.search(regex_tatweel, word)
                elongation_replacement = elongation_found.group()[0]
                elongation_pattern = elongation_found.group()
                word = re.sub(
                    elongation_pattern, elongation_replacement, word, flags=re.MULTILINE
                )
            else:
                break
        return word

    def _remove_redundant_punct(self, text):
        text_ = text
        result = re.search(redundant_punct_pattern, text)
        dif = 0
        while result:
            sub = result.group()
            sub = sorted(set(sub), key=sub.index)
            sub = &#34; &#34; + &#34;&#34;.join(list(sub)) + &#34; &#34;
            text = &#34;&#34;.join(
                (text[: result.span()[0] + dif], sub, text[result.span()[1] + dif :])
            )
            text_ = &#34;&#34;.join(
                (text_[: result.span()[0]], text_[result.span()[1] :])
            ).strip()
            dif = abs(len(text) - len(text_))
            result = re.search(redundant_punct_pattern, text_)
        text = re.sub(r&#34;\s+&#34;, &#34; &#34;, text)
        return text.strip()


prefix_list = [
    &#34;ال&#34;,
    &#34;و&#34;,
    &#34;ف&#34;,
    &#34;ب&#34;,
    &#34;ك&#34;,
    &#34;ل&#34;,
    &#34;لل&#34;,
    &#34;\u0627\u0644&#34;,
    &#34;\u0648&#34;,
    &#34;\u0641&#34;,
    &#34;\u0628&#34;,
    &#34;\u0643&#34;,
    &#34;\u0644&#34;,
    &#34;\u0644\u0644&#34;,
    &#34;س&#34;,
]
suffix_list = [
    &#34;ه&#34;,
    &#34;ها&#34;,
    &#34;ك&#34;,
    &#34;ي&#34;,
    &#34;هما&#34;,
    &#34;كما&#34;,
    &#34;نا&#34;,
    &#34;كم&#34;,
    &#34;هم&#34;,
    &#34;هن&#34;,
    &#34;كن&#34;,
    &#34;ا&#34;,
    &#34;ان&#34;,
    &#34;ين&#34;,
    &#34;ون&#34;,
    &#34;وا&#34;,
    &#34;ات&#34;,
    &#34;ت&#34;,
    &#34;ن&#34;,
    &#34;ة&#34;,
    &#34;\u0647&#34;,
    &#34;\u0647\u0627&#34;,
    &#34;\u0643&#34;,
    &#34;\u064a&#34;,
    &#34;\u0647\u0645\u0627&#34;,
    &#34;\u0643\u0645\u0627&#34;,
    &#34;\u0646\u0627&#34;,
    &#34;\u0643\u0645&#34;,
    &#34;\u0647\u0645&#34;,
    &#34;\u0647\u0646&#34;,
    &#34;\u0643\u0646&#34;,
    &#34;\u0627&#34;,
    &#34;\u0627\u0646&#34;,
    &#34;\u064a\u0646&#34;,
    &#34;\u0648\u0646&#34;,
    &#34;\u0648\u0627&#34;,
    &#34;\u0627\u062a&#34;,
    &#34;\u062a&#34;,
    &#34;\u0646&#34;,
    &#34;\u0629&#34;,
]
other_tokens = [&#34;[رابط]&#34;, &#34;[مستخدم]&#34;, &#34;[بريد]&#34;]

# the never_split list is ussed with the transformers library
prefix_symbols = [x + &#34;+&#34; for x in prefix_list]
suffix_symblos = [&#34;+&#34; + x for x in suffix_list]
never_split_tokens = list(set(prefix_symbols + suffix_symblos + other_tokens))

url_regexes = [
    r&#34;(http(s)?:\/\/.)?(www\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b([-a-zA-Z0-9@:%_\+.~#?&amp;//=]*)&#34;,
    r&#34;@(https?|ftp)://(-\.)?([^\s/?\.#-]+\.?)+(/[^\s]*)?$@iS&#34;,
    r&#34;http[s]?://[a-zA-Z0-9_\-./~\?=%&amp;]+&#34;,
    r&#34;www[a-zA-Z0-9_\-?=%&amp;/.~]+&#34;,
    r&#34;[a-zA-Z]+\.com&#34;,
    r&#34;(?=http)[^\s]+&#34;,
    r&#34;(?=www)[^\s]+&#34;,
    r&#34;://&#34;,
]
user_mention_regex = r&#34;@[\w\d]+&#34;
email_regexes = [r&#34;[\w-]+@([\w-]+\.)+[\w-]+&#34;, r&#34;\S+@\S+&#34;]
redundant_punct_pattern = (
    r&#34;([!\&#34;#\$%\&#39;\(\)\*\+,\.:;\-&lt;=·&gt;?@\[\\\]\^_ـ`{\|}~—٪’،؟`୍“؛”ۚ【»؛\s+«–…‘]{2,})&#34;
)
regex_tatweel = r&#34;(\D)\1{2,}&#34;
rejected_chars_regex = r&#34;[^0-9\u0621-\u063A\u0640-\u066C\u0671-\u0674a-zA-Z\[\]!\&#34;#\$%\&#39;\(\)\*\+,\.:;\-&lt;=·&gt;?@\[\\\]\^_ـ`{\|}~—٪’،؟`୍“؛”ۚ»؛\s+«–…‘]&#34;

regex_url_step1 = r&#34;(?=http)[^\s]+&#34;
regex_url_step2 = r&#34;(?=www)[^\s]+&#34;
regex_url = r&#34;(http(s)?:\/\/.)?(www\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b([-a-zA-Z0-9@:%_\+.~#?&amp;//=]*)&#34;
regex_mention = r&#34;@[\w\d]+&#34;
regex_email = r&#34;\S+@\S+&#34;

chars_regex = r&#34;0-9\u0621-\u063A\u0640-\u066C\u0671-\u0674a-zA-Z\[\]!\&#34;#\$%\&#39;\(\)\*\+,\.:;\-&lt;=·&gt;?@\[\\\]\^_ـ`{\|}~—٪’،؟`୍“؛”ۚ»؛\s+«–…‘&#34;

white_spaced_double_quotation_regex = r&#39;\&#34;\s+([^&#34;]+)\s+\&#34;&#39;
white_spaced_single_quotation_regex = r&#34;\&#39;\s+([^&#39;]+)\s+\&#39;&#34;
white_spaced_back_quotation_regex = r&#34;\`\s+([^`]+)\s+\`&#34;
white_spaced_em_dash = r&#34;\—\s+([^—]+)\s+\—&#34;

left_spaced_chars = r&#34; ([\]!#\$%\),\.:;\?}٪’،؟”؛…»·])&#34;
right_spaced_chars = r&#34;([\[\(\{“«‘*\~]) &#34;
left_and_right_spaced_chars = r&#34; ([\+\-\&lt;\=\&gt;\@\\\^\_\|\–]) &#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Logic.Preparation.preprocess.ArabertPreprocessor"><code class="flex name class">
<span>class <span class="ident">ArabertPreprocessor</span></span>
<span>(</span><span>model_name, keep_emojis=False, remove_html_markup=True, replace_urls_emails_mentions=True)</span>
</code></dt>
<dd>
<div class="desc"><p>A Preprocessor class that cleans and preprocesses text for all models in the AraBERT repo.
It also can unprocess the text ouput of the generated text</p>
<h2 id="args">Args</h2>
<p>model_name (:obj:<code>str</code>): model name from the HuggingFace Models page with or without <code>aubmindlab/</code>. Current accepted models are:</p>
<pre><code>- :obj:`"bert-base-arabertv01"`: No farasa segmentation.
- :obj:`"bert-base-arabert"`: with farasa segmentation.
- :obj:`"bert-base-arabertv02"`: No farasas egmentation.
- :obj:`"bert-base-arabertv2"`: with farasa segmentation.
- :obj:`"bert-large-arabertv02"`: No farasas egmentation.
- :obj:`"bert-large-arabertv2"`: with farasa segmentation.
- :obj:`"araelectra-base"`: No farasa segmentation.
- :obj:`"araelectra-base-discriminator"`: No farasa segmentation.
- :obj:`"araelectra-base-generator"`: No farasa segmentation.
- :obj:`"aragpt2-base"`: No farasa segmentation.
- :obj:`"aragpt2-medium"`: No farasa segmentation.
- :obj:`"aragpt2-large"`: No farasa segmentation.
- :obj:`"aragpt2-mega"`: No farasa segmentation.
</code></pre>
<p>keep_emojis(:obj: <code>bool</code>): don't remove emojis while preprocessing. Defaults to False</p>
<p>remove_html_markup(:obj: <code>bool</code>): Whether to remove html artfacts, should be set to False when preprocessing TyDi QA. Defaults to True</p>
<p>replace_urls_emails_mentions(:obj: <code>bool</code>): Whether to replace email urls and mentions by special tokens. Defaults to True</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="Logic.Preparation.preprocess.ArabertPreprocessor" href="#Logic.Preparation.preprocess.ArabertPreprocessor">ArabertPreprocessor</a></code></dt>
<dd>the preprocessor class</dd>
</dl>
<h2 id="example">Example</h2>
<p>from preprocess import ArabertPreprocessor</p>
<p>arabert_prep = ArabertPreprocessor("bert-base-arabertv2")
arabert_prep.preprocess("SOME ARABIC TEXT")</p>
<p>model_name (:obj:<code>str</code>): model name from the HuggingFace Models page without the aubmindlab tag. Current accepted models are:</p>
<pre><code>- :obj:`"bert-base-arabertv01"`: No farasa segmentation.
- :obj:`"bert-base-arabert"`: with farasa segmentation.
- :obj:`"bert-base-arabertv02"`: No farasas egmentation.
- :obj:`"bert-base-arabertv2"`: with farasa segmentation.
- :obj:`"bert-large-arabertv02"`: No farasas egmentation.
- :obj:`"bert-large-arabertv2"`: with farasa segmentation.
- :obj:`"araelectra-base"`: No farasa segmentation.
- :obj:`"araelectra-base-discriminator"`: No farasa segmentation.
- :obj:`"araelectra-base-generator"`: No farasa segmentation.
- :obj:`"aragpt2-base"`: No farasa segmentation.
- :obj:`"aragpt2-medium"`: No farasa segmentation.
- :obj:`"aragpt2-large"`: No farasa segmentation.
- :obj:`"aragpt2-mega"`: No farasa segmentation.
</code></pre>
<p>keep_emojis(:obj: <code>bool</code>): don't remove emojis while preprocessing. Defaults to False</p>
<p>remove_html_markup(:obj: <code>bool</code>): Whether to remove html artfacts, should be set to False when preprocessing TyDi QA. Defaults to True</p>
<p>replace_urls_emails_mentions(:obj: <code>bool</code>): Whether to replace email urls and mentions by special tokens. Defaults to True</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ArabertPreprocessor:
    &#34;&#34;&#34;
    A Preprocessor class that cleans and preprocesses text for all models in the AraBERT repo.
    It also can unprocess the text ouput of the generated text

    Args:

        model_name (:obj:`str`): model name from the HuggingFace Models page with or without `aubmindlab/`. Current accepted models are:

            - :obj:`&#34;bert-base-arabertv01&#34;`: No farasa segmentation.
            - :obj:`&#34;bert-base-arabert&#34;`: with farasa segmentation.
            - :obj:`&#34;bert-base-arabertv02&#34;`: No farasas egmentation.
            - :obj:`&#34;bert-base-arabertv2&#34;`: with farasa segmentation.
            - :obj:`&#34;bert-large-arabertv02&#34;`: No farasas egmentation.
            - :obj:`&#34;bert-large-arabertv2&#34;`: with farasa segmentation.
            - :obj:`&#34;araelectra-base&#34;`: No farasa segmentation.
            - :obj:`&#34;araelectra-base-discriminator&#34;`: No farasa segmentation.
            - :obj:`&#34;araelectra-base-generator&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-base&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-medium&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-large&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-mega&#34;`: No farasa segmentation.

    keep_emojis(:obj: `bool`): don&#39;t remove emojis while preprocessing. Defaults to False

    remove_html_markup(:obj: `bool`): Whether to remove html artfacts, should be set to False when preprocessing TyDi QA. Defaults to True

    replace_urls_emails_mentions(:obj: `bool`): Whether to replace email urls and mentions by special tokens. Defaults to True

    Returns:

        ArabertPreprocessor: the preprocessor class

    Example:

        from preprocess import ArabertPreprocessor

        arabert_prep = ArabertPreprocessor(&#34;bert-base-arabertv2&#34;)
        arabert_prep.preprocess(&#34;SOME ARABIC TEXT&#34;)
    &#34;&#34;&#34;

    def __init__(
        self,
        model_name,
        keep_emojis=False,
        remove_html_markup=True,
        replace_urls_emails_mentions=True,
    ):
        &#34;&#34;&#34;
        model_name (:obj:`str`): model name from the HuggingFace Models page without the aubmindlab tag. Current accepted models are:

            - :obj:`&#34;bert-base-arabertv01&#34;`: No farasa segmentation.
            - :obj:`&#34;bert-base-arabert&#34;`: with farasa segmentation.
            - :obj:`&#34;bert-base-arabertv02&#34;`: No farasas egmentation.
            - :obj:`&#34;bert-base-arabertv2&#34;`: with farasa segmentation.
            - :obj:`&#34;bert-large-arabertv02&#34;`: No farasas egmentation.
            - :obj:`&#34;bert-large-arabertv2&#34;`: with farasa segmentation.
            - :obj:`&#34;araelectra-base&#34;`: No farasa segmentation.
            - :obj:`&#34;araelectra-base-discriminator&#34;`: No farasa segmentation.
            - :obj:`&#34;araelectra-base-generator&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-base&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-medium&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-large&#34;`: No farasa segmentation.
            - :obj:`&#34;aragpt2-mega&#34;`: No farasa segmentation.

        keep_emojis(:obj: `bool`): don&#39;t remove emojis while preprocessing. Defaults to False

        remove_html_markup(:obj: `bool`): Whether to remove html artfacts, should be set to False when preprocessing TyDi QA. Defaults to True

        replace_urls_emails_mentions(:obj: `bool`): Whether to replace email urls and mentions by special tokens. Defaults to True
        &#34;&#34;&#34;
        model_name = model_name.replace(&#34;aubmindlab/&#34;, &#34;&#34;)

        if model_name not in ACCEPTED_MODELS:
            logging.warning(
                &#34;Model provided is not in the accepted model list. Assuming you don&#39;t want Farasa Segmentation&#34;
            )
            self.model_name = &#34;bert-base-arabertv02&#34;
        else:
            self.model_name = model_name

        if self.model_name in SEGMENTED_MODELS:
            logging.info(
                &#34;Selected Model requires pre-segmentation, Initializing FarasaSegmenter&#34;
            )
            try:
                from farasa.segmenter import FarasaSegmenter

                self.farasa_segmenter = FarasaSegmenter(interactive=True)
            except:
                logging.warning(
                    &#34;farasapy is not installed, you want be able to process text for AraBERTv1 and v2. Install it using: pip install farasapy&#34;
                )
        else:
            logging.info(
                &#34;Selected Model doesn&#39;t require pre-segmentation, skipping FarasaSegmenter initialization&#34;
            )

        self.keep_emojis = keep_emojis
        if self.keep_emojis:
            import emoji

            self.emoji = emoji
            if self.model_name in SEGMENTED_MODELS:
                logging.warning(
                    &#34;Keeping tweets with Farasa Segmentation is 10 times slower&#34;
                )

        self.remove_html_markup = remove_html_markup
        self.replace_urls_emails_mentions = replace_urls_emails_mentions

    def preprocess(self, text):
        &#34;&#34;&#34;
        Preprocess takes an input text line an applies the same preprocessing used in AraBERT
                            pretraining

        Args:

            text (:obj:`str`): inout text string

        Returns:

            string: A preprocessed string depending on which model was selected
        &#34;&#34;&#34;
        if self.model_name == &#34;bert-base-arabert&#34;:
            return self._old_preprocess(
                text,
                do_farasa_tokenization=True,
            )

        if self.model_name == &#34;bert-base-arabertv01&#34;:
            return self._old_preprocess(text, do_farasa_tokenization=False)

        text = str(text)
        text = html.unescape(text)
        text = araby.strip_tashkeel(text)
        text = araby.strip_tatweel(text)

        if self.replace_urls_emails_mentions:
            # replace all possible URLs
            for reg in url_regexes:
                text = re.sub(reg, &#34; [رابط] &#34;, text)
            # REplace Emails with [بريد]
            for reg in email_regexes:
                text = re.sub(reg, &#34; [بريد] &#34;, text)
            # replace mentions with [مستخدم]
            text = re.sub(user_mention_regex, &#34; [مستخدم] &#34;, text)

        if self.remove_html_markup:
            # remove html line breaks
            text = re.sub(&#34;&lt;br /&gt;&#34;, &#34; &#34;, text)
            # remove html markup
            text = re.sub(&#34;&lt;/?[^&gt;]+&gt;&#34;, &#34; &#34;, text)
        # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets
        text = re.sub(
            &#34;([^0-9\u0621-\u063A\u0641-\u064A\u0660-\u0669a-zA-Z\[\]])&#34;, r&#34; \1 &#34;, text
        )

        # insert whitespace between words and numbers or numbers and words
        text = re.sub(
            &#34;(\d+)([\u0621-\u063A\u0641-\u064A\u0660-\u066C]+)&#34;, r&#34; \1 \2 &#34;, text
        )
        text = re.sub(
            &#34;([\u0621-\u063A\u0641-\u064A\u0660-\u066C]+)(\d+)&#34;, r&#34; \1 \2 &#34;, text
        )

        # remove unwanted characters
        if self.keep_emojis:
            emoji_regex = &#34;&#34;.join(list(self.emoji.UNICODE_EMOJI[&#34;en&#34;].keys()))
            rejected_chars_regex2 = &#34;[^%s%s]&#34; % (chars_regex, emoji_regex)
            text = re.sub(rejected_chars_regex2, &#34; &#34;, text)
        else:
            text = re.sub(rejected_chars_regex, &#34; &#34;, text)

        # remove repeated characters &gt;2
        text = self._remove_elongation(text)
        # remove extra spaces
        text = &#34; &#34;.join(text.replace(&#34;\uFE0F&#34;, &#34;&#34;).split())

        if (
            self.model_name == &#34;bert-base-arabertv2&#34;
            or self.model_name == &#34;bert-large-arabertv2&#34;
        ):
            if self.keep_emojis:
                new_text = []
                for word in text.split():
                    if word in list(self.emoji.UNICODE_EMOJI[&#34;en&#34;].keys()):
                        new_text.append(word)
                    else:
                        new_text.append(self.farasa_segmenter.segment(word))
                text = &#34; &#34;.join(new_text)
            else:
                text = self.farasa_segmenter.segment(text)
            return self._farasa_segment(text)

        # ALl the other models dont require Farasa Segmentation
        return text

    def unpreprocess(self, text, desegment=True):
        &#34;&#34;&#34;Re-formats the text to a classic format where punctuations, brackets, parenthesis are not seperated by whitespaces.
        The objective is to make the generated text of any model appear natural and not preprocessed.

        Args:
            text (str): input text to be un-preprocessed
            desegment (bool, optional): [whether or not to remove farasa pre-segmentation before]. Defaults to True.

        Returns:
            str: The unpreprocessed (and possibly Farasa-desegmented) text.
        &#34;&#34;&#34;

        if self.model_name in SEGMENTED_MODELS and desegment:
            text = self.desegment(text)

        # removes the spaces around quotation marks ex: i &#34; ate &#34; an apple --&gt; i &#34;ate&#34; an apple
        # https://stackoverflow.com/a/53436792/5381220
        text = re.sub(white_spaced_double_quotation_regex, &#39;&#34;&#39; + r&#34;\1&#34; + &#39;&#34;&#39;, text)
        text = re.sub(white_spaced_single_quotation_regex, &#34;&#39;&#34; + r&#34;\1&#34; + &#34;&#39;&#34;, text)
        text = re.sub(white_spaced_back_quotation_regex, &#34;\`&#34; + r&#34;\1&#34; + &#34;\`&#34;, text)
        text = re.sub(white_spaced_back_quotation_regex, &#34;\—&#34; + r&#34;\1&#34; + &#34;\—&#34;, text)

        # during generation, sometimes the models don&#39;t put a space after the dot, this handles it
        text = text.replace(&#34;.&#34;, &#34; . &#34;)
        text = &#34; &#34;.join(text.split())

        # handle decimals
        text = re.sub(r&#34;(\d+) \. (\d+)&#34;, r&#34;\1.\2&#34;, text)
        text = re.sub(r&#34;(\d+) \, (\d+)&#34;, r&#34;\1,\2&#34;, text)

        text = re.sub(left_and_right_spaced_chars, r&#34;\1&#34;, text)
        text = re.sub(left_spaced_chars, r&#34;\1&#34;, text)
        text = re.sub(right_spaced_chars, r&#34;\1&#34;, text)

        return text

    def desegment(self, text):
        &#34;&#34;&#34;
        Use this function if sentence tokenization was done using
        `from arabert.preprocess_arabert import preprocess` with Farasa enabled
        AraBERT segmentation using Farasa adds a space after the &#39;+&#39; for prefixes,
        and after before the &#39;+&#39; for suffixes

        Example:
        &gt;&gt;&gt; desegment(&#39;ال+ دراس +ات&#39;)
        الدراسات
        &#34;&#34;&#34;
        text = text.replace(&#34;+ &#34;, &#34;+&#34;)
        text = text.replace(&#34; +&#34;, &#34;+&#34;)
        text = &#34; &#34;.join([self._desegmentword(word) for word in text.split(&#34; &#34;)])
        return text

    def _desegmentword(self, orig_word: str) -&gt; str:
        &#34;&#34;&#34;
        Word segmentor that takes a Farasa Segmented Word and removes the &#39;+&#39; signs

        Example:
        &gt;&gt;&gt; _desegmentword(&#34;ال+يومي+ة&#34;)
        اليومية
        &#34;&#34;&#34;
        word = orig_word.replace(&#34;ل+ال+&#34;, &#34;لل&#34;)
        if &#34;ال+ال&#34; not in orig_word:
            word = word.replace(&#34;ل+ال&#34;, &#34;لل&#34;)
        word = word.replace(&#34;+&#34;, &#34;&#34;)
        word = word.replace(&#34;للل&#34;, &#34;لل&#34;)
        return word

    def _old_preprocess(self, text, do_farasa_tokenization):
        &#34;&#34;&#34;
        AraBERTv1 preprocessing Function
        &#34;&#34;&#34;
        text = str(text)
        text = araby.strip_tashkeel(text)
        text = re.sub(r&#34;\d+\/[ء-ي]+\/\d+\]&#34;, &#34;&#34;, text)
        text = re.sub(&#34;ـ&#34;, &#34;&#34;, text)
        text = re.sub(&#34;[«»]&#34;, &#39; &#34; &#39;, text)

        if self.replace_urls_emails_mentions:
            # replace the [رابط] token with space if you want to clean links
            text = re.sub(regex_url_step1, &#34;[رابط]&#34;, text)
            text = re.sub(regex_url_step2, &#34;[رابط]&#34;, text)
            text = re.sub(regex_url, &#34;[رابط]&#34;, text)
            text = re.sub(regex_email, &#34;[بريد]&#34;, text)
            text = re.sub(regex_mention, &#34;[مستخدم]&#34;, text)
        text = re.sub(&#34;…&#34;, r&#34;arabert&#34;, text).strip()
        text = self._remove_redundant_punct(text)

        if self.replace_urls_emails_mentions:
            text = re.sub(r&#34;\[ رابط \]|\[ رابط\]|\[رابط \]&#34;, &#34; [رابط] &#34;, text)
            text = re.sub(r&#34;\[ بريد \]|\[ بريد\]|\[بريد \]&#34;, &#34; [بريد] &#34;, text)
            text = re.sub(r&#34;\[ مستخدم \]|\[ مستخدم\]|\[مستخدم \]&#34;, &#34; [مستخدم] &#34;, text)

        text = self._remove_elongation(text)
        text = re.sub(
            &#34;([^0-9\u0621-\u063A\u0641-\u0669\u0671-\u0673a-zA-Z\[\]])&#34;, r&#34; \1 &#34;, text
        )
        if do_farasa_tokenization:
            text = self._tokenize_arabic_words_farasa(text)

        return text.strip()

    def _farasa_segment(self, text):
        line_farasa = text.split()
        segmented_line = []
        for index, word in enumerate(line_farasa):
            if word in [&#34;[&#34;, &#34;]&#34;]:
                continue
            if word in [&#34;رابط&#34;, &#34;بريد&#34;, &#34;مستخدم&#34;] and line_farasa[index - 1] in [
                &#34;[&#34;,
                &#34;]&#34;,
            ]:
                segmented_line.append(&#34;[&#34; + word + &#34;]&#34;)
                continue
            if &#34;+&#34; not in word:
                segmented_line.append(word)
                continue
            segmented_word = self._split_farasa_output(word)
            segmented_line.extend(segmented_word)

        return &#34; &#34;.join(segmented_line)

    def _split_farasa_output(self, word):
        segmented_word = []
        temp_token = &#34;&#34;
        for i, c in enumerate(word):
            if c == &#34;+&#34;:
                # if the token is KAF, it could be a suffix or prefix
                if temp_token == &#34;ك&#34;:
                    # if we are at the second token, then KAF is surely a prefix
                    if i == 1:
                        segmented_word.append(temp_token + &#34;+&#34;)
                        temp_token = &#34;&#34;
                    # If the KAF token is between 2 tokens
                    elif word[i - 2] == &#34;+&#34;:
                        # if the previous token is prefix, then this KAF must be a prefix
                        if segmented_word[-1][-1] == &#34;+&#34;:
                            segmented_word.append(temp_token + &#34;+&#34;)
                            temp_token = &#34;&#34;
                        # else it is a suffix, this KAF could not be a second suffix
                        else:
                            segmented_word.append(&#34;+&#34; + temp_token)
                            temp_token = &#34;&#34;
                    # if Kaf is at the end, this is handled with the statement after the loop
                elif temp_token in prefix_list:
                    segmented_word.append(temp_token + &#34;+&#34;)
                    temp_token = &#34;&#34;
                elif temp_token in suffix_list:
                    segmented_word.append(&#34;+&#34; + temp_token)
                    temp_token = &#34;&#34;
                else:
                    segmented_word.append(temp_token)
                    temp_token = &#34;&#34;
                continue
            temp_token += c
        if temp_token != &#34;&#34;:
            if temp_token in suffix_list:
                segmented_word.append(&#34;+&#34; + temp_token)
            else:
                segmented_word.append(temp_token)
        return segmented_word

    def _tokenize_arabic_words_farasa(self, line_input):

        if self.keep_emojis:
            # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets
            line_farasa = []
            for word in line_input.split():
                if word in list(self.emoji.UNICODE_EMOJI[&#34;en&#34;].keys()):
                    line_farasa.append(word)
                else:
                    line_farasa.append(self.farasa_segmenter.segment(word))
        else:
            line_farasa = self.farasa_segmenter.segment(line_input).split()

        segmented_line = []
        for index, word in enumerate(line_farasa):
            if word in [&#34;[&#34;, &#34;]&#34;]:
                continue
            if word in [&#34;رابط&#34;, &#34;بريد&#34;, &#34;مستخدم&#34;] and line_farasa[index - 1] in [
                &#34;[&#34;,
                &#34;]&#34;,
            ]:
                segmented_line.append(&#34;[&#34; + word + &#34;]&#34;)
                continue
            segmented_word = []
            for token in word.split(&#34;+&#34;):
                if token in prefix_list:
                    segmented_word.append(token + &#34;+&#34;)
                elif token in suffix_list:
                    segmented_word.append(&#34;+&#34; + token)
                else:
                    segmented_word.append(token)
            segmented_line.extend(segmented_word)
        return &#34; &#34;.join(segmented_line)

    def _remove_elongation(self, word):
        &#34;&#34;&#34;
        :param word:  the input word to remove elongation
        :return: delongated word
        &#34;&#34;&#34;
        # loop over the number of times the regex matched the word
        for index_ in range(len(re.findall(regex_tatweel, word))):
            if re.search(regex_tatweel, word):
                elongation_found = re.search(regex_tatweel, word)
                elongation_replacement = elongation_found.group()[0]
                elongation_pattern = elongation_found.group()
                word = re.sub(
                    elongation_pattern, elongation_replacement, word, flags=re.MULTILINE
                )
            else:
                break
        return word

    def _remove_redundant_punct(self, text):
        text_ = text
        result = re.search(redundant_punct_pattern, text)
        dif = 0
        while result:
            sub = result.group()
            sub = sorted(set(sub), key=sub.index)
            sub = &#34; &#34; + &#34;&#34;.join(list(sub)) + &#34; &#34;
            text = &#34;&#34;.join(
                (text[: result.span()[0] + dif], sub, text[result.span()[1] + dif :])
            )
            text_ = &#34;&#34;.join(
                (text_[: result.span()[0]], text_[result.span()[1] :])
            ).strip()
            dif = abs(len(text) - len(text_))
            result = re.search(redundant_punct_pattern, text_)
        text = re.sub(r&#34;\s+&#34;, &#34; &#34;, text)
        return text.strip()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Logic.Preparation.preprocess.ArabertPreprocessor.desegment"><code class="name flex">
<span>def <span class="ident">desegment</span></span>(<span>self, text)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this function if sentence tokenization was done using
<code>from arabert.preprocess_arabert import preprocess</code> with Farasa enabled
AraBERT segmentation using Farasa adds a space after the '+' for prefixes,
and after before the '+' for suffixes</p>
<p>Example:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; desegment('ال+ دراس +ات')
الدراسات
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def desegment(self, text):
    &#34;&#34;&#34;
    Use this function if sentence tokenization was done using
    `from arabert.preprocess_arabert import preprocess` with Farasa enabled
    AraBERT segmentation using Farasa adds a space after the &#39;+&#39; for prefixes,
    and after before the &#39;+&#39; for suffixes

    Example:
    &gt;&gt;&gt; desegment(&#39;ال+ دراس +ات&#39;)
    الدراسات
    &#34;&#34;&#34;
    text = text.replace(&#34;+ &#34;, &#34;+&#34;)
    text = text.replace(&#34; +&#34;, &#34;+&#34;)
    text = &#34; &#34;.join([self._desegmentword(word) for word in text.split(&#34; &#34;)])
    return text</code></pre>
</details>
</dd>
<dt id="Logic.Preparation.preprocess.ArabertPreprocessor.preprocess"><code class="name flex">
<span>def <span class="ident">preprocess</span></span>(<span>self, text)</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocess takes an input text line an applies the same preprocessing used in AraBERT
pretraining</p>
<h2 id="args">Args</h2>
<p>text (:obj:<code>str</code>): inout text string</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>string</code></dt>
<dd>A preprocessed string depending on which model was selected</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess(self, text):
    &#34;&#34;&#34;
    Preprocess takes an input text line an applies the same preprocessing used in AraBERT
                        pretraining

    Args:

        text (:obj:`str`): inout text string

    Returns:

        string: A preprocessed string depending on which model was selected
    &#34;&#34;&#34;
    if self.model_name == &#34;bert-base-arabert&#34;:
        return self._old_preprocess(
            text,
            do_farasa_tokenization=True,
        )

    if self.model_name == &#34;bert-base-arabertv01&#34;:
        return self._old_preprocess(text, do_farasa_tokenization=False)

    text = str(text)
    text = html.unescape(text)
    text = araby.strip_tashkeel(text)
    text = araby.strip_tatweel(text)

    if self.replace_urls_emails_mentions:
        # replace all possible URLs
        for reg in url_regexes:
            text = re.sub(reg, &#34; [رابط] &#34;, text)
        # REplace Emails with [بريد]
        for reg in email_regexes:
            text = re.sub(reg, &#34; [بريد] &#34;, text)
        # replace mentions with [مستخدم]
        text = re.sub(user_mention_regex, &#34; [مستخدم] &#34;, text)

    if self.remove_html_markup:
        # remove html line breaks
        text = re.sub(&#34;&lt;br /&gt;&#34;, &#34; &#34;, text)
        # remove html markup
        text = re.sub(&#34;&lt;/?[^&gt;]+&gt;&#34;, &#34; &#34;, text)
    # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets
    text = re.sub(
        &#34;([^0-9\u0621-\u063A\u0641-\u064A\u0660-\u0669a-zA-Z\[\]])&#34;, r&#34; \1 &#34;, text
    )

    # insert whitespace between words and numbers or numbers and words
    text = re.sub(
        &#34;(\d+)([\u0621-\u063A\u0641-\u064A\u0660-\u066C]+)&#34;, r&#34; \1 \2 &#34;, text
    )
    text = re.sub(
        &#34;([\u0621-\u063A\u0641-\u064A\u0660-\u066C]+)(\d+)&#34;, r&#34; \1 \2 &#34;, text
    )

    # remove unwanted characters
    if self.keep_emojis:
        emoji_regex = &#34;&#34;.join(list(self.emoji.UNICODE_EMOJI[&#34;en&#34;].keys()))
        rejected_chars_regex2 = &#34;[^%s%s]&#34; % (chars_regex, emoji_regex)
        text = re.sub(rejected_chars_regex2, &#34; &#34;, text)
    else:
        text = re.sub(rejected_chars_regex, &#34; &#34;, text)

    # remove repeated characters &gt;2
    text = self._remove_elongation(text)
    # remove extra spaces
    text = &#34; &#34;.join(text.replace(&#34;\uFE0F&#34;, &#34;&#34;).split())

    if (
        self.model_name == &#34;bert-base-arabertv2&#34;
        or self.model_name == &#34;bert-large-arabertv2&#34;
    ):
        if self.keep_emojis:
            new_text = []
            for word in text.split():
                if word in list(self.emoji.UNICODE_EMOJI[&#34;en&#34;].keys()):
                    new_text.append(word)
                else:
                    new_text.append(self.farasa_segmenter.segment(word))
            text = &#34; &#34;.join(new_text)
        else:
            text = self.farasa_segmenter.segment(text)
        return self._farasa_segment(text)

    # ALl the other models dont require Farasa Segmentation
    return text</code></pre>
</details>
</dd>
<dt id="Logic.Preparation.preprocess.ArabertPreprocessor.unpreprocess"><code class="name flex">
<span>def <span class="ident">unpreprocess</span></span>(<span>self, text, desegment=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Re-formats the text to a classic format where punctuations, brackets, parenthesis are not seperated by whitespaces.
The objective is to make the generated text of any model appear natural and not preprocessed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>str</code></dt>
<dd>input text to be un-preprocessed</dd>
<dt><strong><code>desegment</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[whether or not to remove farasa pre-segmentation before]. Defaults to True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The unpreprocessed (and possibly Farasa-desegmented) text.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unpreprocess(self, text, desegment=True):
    &#34;&#34;&#34;Re-formats the text to a classic format where punctuations, brackets, parenthesis are not seperated by whitespaces.
    The objective is to make the generated text of any model appear natural and not preprocessed.

    Args:
        text (str): input text to be un-preprocessed
        desegment (bool, optional): [whether or not to remove farasa pre-segmentation before]. Defaults to True.

    Returns:
        str: The unpreprocessed (and possibly Farasa-desegmented) text.
    &#34;&#34;&#34;

    if self.model_name in SEGMENTED_MODELS and desegment:
        text = self.desegment(text)

    # removes the spaces around quotation marks ex: i &#34; ate &#34; an apple --&gt; i &#34;ate&#34; an apple
    # https://stackoverflow.com/a/53436792/5381220
    text = re.sub(white_spaced_double_quotation_regex, &#39;&#34;&#39; + r&#34;\1&#34; + &#39;&#34;&#39;, text)
    text = re.sub(white_spaced_single_quotation_regex, &#34;&#39;&#34; + r&#34;\1&#34; + &#34;&#39;&#34;, text)
    text = re.sub(white_spaced_back_quotation_regex, &#34;\`&#34; + r&#34;\1&#34; + &#34;\`&#34;, text)
    text = re.sub(white_spaced_back_quotation_regex, &#34;\—&#34; + r&#34;\1&#34; + &#34;\—&#34;, text)

    # during generation, sometimes the models don&#39;t put a space after the dot, this handles it
    text = text.replace(&#34;.&#34;, &#34; . &#34;)
    text = &#34; &#34;.join(text.split())

    # handle decimals
    text = re.sub(r&#34;(\d+) \. (\d+)&#34;, r&#34;\1.\2&#34;, text)
    text = re.sub(r&#34;(\d+) \, (\d+)&#34;, r&#34;\1,\2&#34;, text)

    text = re.sub(left_and_right_spaced_chars, r&#34;\1&#34;, text)
    text = re.sub(left_spaced_chars, r&#34;\1&#34;, text)
    text = re.sub(right_spaced_chars, r&#34;\1&#34;, text)

    return text</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Logic.Preparation" href="index.html">Logic.Preparation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Logic.Preparation.preprocess.ArabertPreprocessor" href="#Logic.Preparation.preprocess.ArabertPreprocessor">ArabertPreprocessor</a></code></h4>
<ul class="">
<li><code><a title="Logic.Preparation.preprocess.ArabertPreprocessor.desegment" href="#Logic.Preparation.preprocess.ArabertPreprocessor.desegment">desegment</a></code></li>
<li><code><a title="Logic.Preparation.preprocess.ArabertPreprocessor.preprocess" href="#Logic.Preparation.preprocess.ArabertPreprocessor.preprocess">preprocess</a></code></li>
<li><code><a title="Logic.Preparation.preprocess.ArabertPreprocessor.unpreprocess" href="#Logic.Preparation.preprocess.ArabertPreprocessor.unpreprocess">unpreprocess</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>